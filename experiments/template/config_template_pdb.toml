[experiments]
# the name of this experiment
exp_name = "{{ exp_name }}"
# seed of the experiment
seed = {{ random_seed }}
# if toggled, `torch.backends.cudnn.deterministic=false`
torch_deterministic = true
# if toggled, cuda will be enabled by default
cuda = false
# if toggled, this experiment will be tracked with Weights and Biases
track = false
# the wandb's project name
wandb_project_name = "{{ exp_name }}"
# the entity (team) of wandb's project
wandb_entity = ""
# whether to capture videos of the agent performances (check out `videos` folder)
capture_video = false
# whether to save model into the `runs/{run_name}` folder
save_model = false
# whether to upload the saved model to huggingface
upload_model = false
# the user or org name of the model repository from the Hugging Face Hub
hf_entity = ""
# The number of top policies to save
num_of_top_policies = 1

[algorithm.general]
# the environment id of the Atari game
env_id = "{{ env_id }}"
# total timesteps of the experiments
total_timesteps = 50_000
# total timesteps of the prediction
predicted_timesteps = 10_000
# the number of steps per episode
max_steps_per_episode = 500
# timestep to start learning
learning_starts = 48
# the replay memory buffer size
buffer_size = 50_000
# the batch size of sample from the reply memory
batch_size = 48
# the discount factor gamma
gamma = 0.99
# the pretrain flag for the actor
actor_pretrain = true
# the number of data for the actor pretrain
actor_pretrain_num_data = 1000
# the covariance's magnification of the mock data for the actor pretrain
actor_pretrain_mag = 10.0
# the number of epochs for the actor pretrain
actor_pretrain_num_epochs = 100
# the batch size for the actor pretrain
actor_pretrain_batch_size = 16
# the flag for shuffling the mock data for the actor pretrain
actor_pretrain_shuffle = true
# the learning rate of the optimizer for the actor
actor_learning_rate = {{ actor_learning_rate }}
# the learning rate of the optimizer for the critic
critic_learning_rate = 1e-2
# the gradient clipping for the actor
actor_gradient_clipping = false
# the gradient threshold for the actor
actor_gradient_threshold = 1.0
# the gradient clipping norm for the actor
actor_gradient_norm = 2
# the gradient clipping for the critic
critic_gradient_clipping = false
# the gradient threshold for the critic
critic_gradient_threshold = 1.0
# the gradient clipping norm for the critic
critic_gradient_norm = 2

[algorithm.specific]
# target smoothing coefficient (default.005)
tau = 0.005
# the scale of policy noise
policy_noise = 0.0
# the scale of exploration noise
exploration_noise = {{ exploration_noise }}
# the frequency of training policy (delayed)
policy_frequency = 2
# noise clip parameter of the Target Policy Smoothing Regularization
noise_clip = 0.5
